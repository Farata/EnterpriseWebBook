[[tdd_with_javasctipt]]
== Test-Driven Development with JavaScript

To shorten the development cycle of your web application, you need to start testing it in the early stages of the project. It seems obvious, but many enterprise IT organizations haven't adopted agile testing methodologies, which costs them dearly. JavaScript is a dynamically typed interpreted language--there is no compiler to help identify errors as is done in compiled languages such as Java. This means that a lot more time should be allocated for testing JavaScript web applications. Moreover, a programmer who doesn't introduce testing techniques into his daily routine can't be 100 percent sure that his code works properly.

The static code analysis and code quality tools such as http://esprima.org/[Esprima] and http://www.jshint.com/[JSHint] will help reduce the number of syntax errors and improve the quality of your code. 

TIP: We demonstrate how to set up JSHint for your JavaScript project and automate the process of checking your code for syntax errors in <<productivity_tools>>.

To switch to a test-driven development mode, make testing part of your development process in its early stages rather than scheduling testing after the development cycle is complete. Introducing test-driven development can substantially improve your code quality. It is important to receive feedback about your code on a regular basis. That's why tests must be automated and should run as soon as you've changed the code. 

There are many testing frameworks in the JavaScript world, but we'll give you a brief overview of two of them: http://qunitjs.com/[QUnit] and http://pivotal.github.com/jasmine/[Jasmine]. The main goal of each framework is to test small  pieces of code, a.k.a. _units_.

We will go through basic testing techniques known as test-driven development and Test First. You'll learn how to automate the testing process in multiple browsers with https://github.com/airportyh/testem[Testem Runner] or by running tests in so called _headless_ mode with http://phantomjs.org/[PhantomJS].

The second part of this chapter is dedicated to setting up a new Save The Child project in the IDE with selected test frameworks.

=== Why Test?

All software has bugs. But in interpreted languages such as JavaScript, you don't have the help of compilers that could point out potential issues in the early stages of development. You need to continue testing code over and over again to catch regression errors, to be able to add new features without breaking the existing ones. Code that is covered with tests is easy to refactor. Tests help prove the correctness of your code. Well-tested code leads to better overall design of your programs.

=== Testing Basics

This chapter covers the following types of testing: 

- Unit testing
- Integration testing
- Functional testing
- Load (a.k.a. stress) testing

[[QA_UAT]]
.Quality Assurance versus Use Acceptance Testing
****
Although, _quality assurance_ (QA) and _user acceptance testing_ (UAT) are far beyond the scope of this chapter, you need understand their differences.

Software QA (a.k.a. _quality control_, or QC) is a process that helps identify the correctness, completeness, security compliance, and quality of the software. QA testing _is performed by specialists_ (testers, analysts). The goal of QA testing is to ensure that the application complies with a set of the predefined behavior requirements.

UAT _is performed by business users_ or subject-area experts. UAT should result in an endorsement that the tested applications/functionality/module meets the agreed-upon requirements. The results of UAT give the confidence to the end user that the system will perform in production according to specifications.

During the QA process, the specialist intends to perform all tests, trying to break the application. This approach helps find errors undiscovered by developers. On the contrary, during UAT the user runs business-as-usual scenarios and makes sure that business functions are implemented in the application.
****

Let's go over the strategies, approaches, and tools that will help you in test automation.

==== Unit Testing

A _unit test_ is a piece of code that invokes a method being tested. It _asserts_ some assumptions about the application logic and behavior of the  method. Typically, you'll write such tests by using a unit-testing framework of your choice. Tests should run fast and be automated with clear output. For example, you can test that if a function is called with particular arguments, it should return an expected result.
We take a closer look at unit-testing terminology and vocabulary in <<TDD>>.

==== Integration Testing

_Integration testing_ is a phase in which already tested units are combined into a module to test the interfaces between them. You might want to test the integration of your code with the code written by other developers; for example, a third-party framework. Integration tests ensure that any abstraction layers we build over the third-party code work as expected. Both unit and integration tests are written by application developers. 

==== Functional Testing

_Functional_ testing is aimed at finding out whether the application properly implements business logic. For example, if the user clicks a row in a grid with customers, the program should display a form view with specific details about the selected customer. In functional testing, business users should define what has to be tested, unlike unit or integration testing, for which tests are created by software developers. 

Functional tests can be performed manually by a real person clicking through each and every view of the web application, confirming that it operates properly or reporting discrepancies with the functional specifications. But there are tools to automate the process of functional testing of Web applications. Such tools allow you to record users' actions and replay them in automatic mode. The following are brief descriptions of two of such tools--Selenium and CasperJS: 

http://docs.seleniumhq.org/[Selenium]::
    Selenium is an advanced browser automation tool suite that has capabilities to run and record user scenarios without requiring developers to learn any scripting languages. Also Selenium has an API for integration with many programming languages such as Java, C#, and JavaScript. Selenium uses the WebDriver API to talk to browsers and receive running context information. WebDriver is becoming https://dvcs.w3.org/hg/webdriver/raw-file/default/webdriver-spec.html[the standard API for browser automation]. Selenium supports a wide range of http://docs.seleniumhq.org/about/platforms.jsp[browsers and platforms].

http://casperjs.org/quickstart.html[Casper.js]::
    CasperJS is a scripting framework written in JavaScript. CasperJS allows you to create interaction scenarios such as defining and ordering navigation steps, filling and submitting forms, or even scrapping web content and making web page screenshots. CasperJS works on top of PhantomJS and SlimerJS browsers, which limits the testing runtime environment to WebKit-based and Gecko-based browsers. Still, it's a useful tool when you want to run tests in a continuous integration (CI) environment.

[[WHAT_IS_PHANTOMJS]]
.What Is PhantomJS and SlimerJS?
****
_PhantomJS_ is a headless WebKit-based rendering engine and interpreter with a JavaScript API. Think of PhantomJS as a browser that doesn't have any graphical user interface. PhantomJS can execute HTML, CSS, and JavaScript code. Because PhantomJS is not required to render a browser's GUI, it can be used in display-less environments (for example, a CI server) to run tests. _SlimerJS_ follows the same idea of a headless browser, similar to PhantomJS, but it uses the Gecko engine, instead.

PhantomsJS is built on top of WebKit and http://trac.webkit.org/wiki/JavaScriptCore[JavaScriptCore] (like Safari), and SlimerJS is built on top of Gecko and https://developer.mozilla.org/en-US/docs/Mozilla/Projects/SpiderMonkey[SpiderMonkey] (like Firefox). You can find a comprehensive list of differences between PhantonJS and SlimerJS APIs in http://docs.slimerjs.org/current/differences-with-phantomjs.html[SlimerJS's documentation site].

In our case, Grunt automatically spawns the PhantomJS instance, executes the code of our tests, reads the execution results using the PhantomJS API, and prints them out in the console. If you're not familiar with Grunt tasks, refer to <<appendix_a>> for additional information about using Grunt in our Save The Child project.
****


==== Load Testing

_Load testing_ is a process that can help answer the following questions:

* How many concurrent users can work with your application without bringing your server to its knees? 

* Even if your server is capable of serving a thousand users, is your application performance in compliance with the service-level agreement (SLA), if any? 

It all comes down to two factors: availability and response time of your application. Ideally, these requirements should be well defined in the SLA document, which should clearly state what metrics are acceptable from the user's perspective. For example, the SLA can include a clause stating that the initial download of your application shouldn't take longer than 10 seconds for users with a slow connection (under 1 Mbps). An SLA can also state that the query to display a list of customers shouldn't run for more than 5 seconds, and the application should be operational 99.9 percent of the time.

To avoid surprises after going live with your new mission-critical web application, don't forget to include in your project plan an item to create and run a set of heavy stress tests. Do this well in advance, before your project goes live. With load testing, you don't need to hire a thousand interns to play the roles of concurrent users to find out whether your application will meet the SLA.

Automated load-testing software allows you to emulate the required number of users, set up throttling to emulate a slower connection, and configure the ramp-up speed. For example, you can simulate a situation in which the number of users logged on to your system grows at the speed of 50 users every 10 seconds. Stress-testing software also allows you to prerecord user interactions, and then you can run these scripts emulating a heavy load.

Professional stress-testing software allows simulating the load close to real-world usage patterns. You should be able to create and run mixed scripts simulating a situation in which some users are logging on to your application, while others are retrieving the data and performing data modifications. The following are some tools worth considering for load testing.

==== http://httpd.apache.org/docs/2.2/programs/ab.html[Apache Benchmark]

Apache Benchmark is a simple to use command line tool. For example, with a command `ab -n 10 -c 10 -t 60 http://savesickchild.org:8080/ssc_extjs/` Apache Benchmark will open 10 concurrent connection with the server and will send 10 requests via each connection to simulate 10 visitors working with your Web application during 60 seconds. The number of concurrent connections is the actual number of concurrent users. You can find an Apache Benchmark sample report in <<AB_REPORT,following snippet>>.

[[AB_REPORT]]
.A sample Apache Benchmark report
====
[source,bash]
----
This is ApacheBench, Version 2.3 <$Revision: 655654 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/
Server Software:    GlassFish
Server Hostname:    savesickchild.org
Server Port:    8080
Document Path:  /ssc_extjs/
Document Length:    306 bytes
Concurrency Level:  10
Time taken for tests:   60.003 seconds
Complete requests:  17526
Failed requests:    0
Total transferred:  11988468 bytes
HTML transferred:   5363262 bytes
Requests per second:    292086.73
Transfer rate:  199798.72 kb/s received
Connnection Times (ms)
             min avg max
Connect:     10  13  1305
Processing:  11  14  12
Total:       21  27  1317
----
====


* http://jmeter.apache.org/[jMeter]
+

Apache JMeter is a tool with a graphic user interface. It can be used to simulate heavy load on a server, network or an object to test its strength or to analyze overall performance under different load types. You can find more about testing web applications using JMeter in http://jmeter.apache.org/usermanual/build-web-test-plan.html[official documentation].

.JMeter test results output example
image::images/ewdv_0701.png[]

* http://phantomjs.org/[PhantomJS]
+

Please, refer the sidebar called <<WHAT_IS_PHANTOMJS,What is PhantomJS>> to make yourself familiar with this tool. The slide deck titled http://wesleyhales.com/slides/html5devconf2013/["Browser Performance metering with PhantomJS"] is yet another good resource for seeing how  PhantomJS can be used for performance testing.

[[TDD]]
=== Test Driven Development

The methodology known as Test-Driven Development (TDD) substantially changes the way a traditional software development is done. This methodology wants you to write tests _even before_ writing the application code. Instead of just using testing to verify our work _after it's done_, TDD moves the testing into the earlier application design phase. You should use the tests to clarify your ideas about what you are about to program. Here is fundamental mantra of TDD:

- Write a test and make it fail.
- Make the test pass.
- Refactor.
- Repeat.

This technique also referred as "Red-Green-Refactor" because IDE's and test runners use red color to indicate failed tests and green color to indicate the tests that passed.

When you are about to start programming a class with some business logic, ask yourself, "How can I ensure that this function works as expected?" After you know the answer, write a test JavaScript class that calls this function _to assert_ that the business logic gives the expected result. 

An assertion is a true-false statement that represents what a programmer assumes about program state, e.g.  *customerID >0* is an assertion. 

According to <<fowler, 'Martin Fowler'>>, an assertion is a section of code that works only if certain conditions are true. It's a conditional statement that is assumed to be always true. Failure of an assertion results in test failure. 

Run your test, and it will immediately fail because no application code is written yet! Only after the test is written, start programming the business logic of your application 

You should write a simplest possible piece of code to make the test pass. Don't try to find a generic solution at this step. For example, if you want to test a calculator that needs to return `4` as result of `2+2` write the code what simply returns `4`. Don't worry about the performance or optimization at this point in time. Just make the test pass. Once you made it, you can refactor your application code to make it more efficient. Now you might want to introduce a real algorithm for implementing the application logic without worrying about breaking the  contract with other components of your application. 

A failed unit test indicates that your code change introduced _regression_, which is a new bug in a previously worked software. Automated testing and well-written test cases can reduce the likelihood of a regression in your code.

TDD allows to receive feedback from your code almost immediately. It's better to find that something is broken during development rather than in the application deployed in production.

[NOTE]
===============================
Learn by heart The Golden Rule Of TDD:
____
Never write new functionality without a failing test
____
===============================

In addition to business logic, web applications should be tested for proper rendering of UI components, changing view states, dispatching, and handling events.

With any testing framework, your tests will follow same basic pattern. First, you need to setup up the test environment. Second, you run the  production code and check that it works as it supposed to. Finally, you need to clean up after the test will run - remove everything that your program has created during setup of the environment.

[[AAAR]]This pattern for authoring unit tests is called _Arrange-Act-Assert-Reset_ (AAAR footnote:[ http://integralpath.blogs.com/thinkingoutloud/2005/09/principles_of_t.html[Principles for Test-Driven Development] ]).

* In the _Arrange_ phase you set up the unit of work to test. For example, create a JavaScript objects, prepare dependencies and etc.
* In the _Act_ phase you exercise the unit under test and capture the  resulting state. You execute your production code in unit test context.
* In the _Assert_ phase you verify the behavior through assertions.
* In the _Reset_ phase, you reset the environment to the initial state. For example, erase the DOM elements created in the _Arrange_ phase. Most of the frameworks provide a "teardown" function that would be invoked after test is done.

Later in this chapter, you'll see how different frameworks implements _AAAR_ pattern.

In next sections we will dive into the testing frameworks for JavaScript.

==== Test-Driven Development With QUnit

We'll start our journey to JavaScript testing frameworks with http://qunitjs.com/[QUnit], which was originally developed by http://ejohn.org/about/[John Resig] as part of jQuery. QUnit now runs completely standalone and doesn't have any jQuery dependencies. While it's still being used by the jQuery Project itself for testing jQuery, jQuery UI and jQuery Mobile code, QUnit can be used to test any generic JavaScript code.

===== Setup Grunt With Qunit

In this section you're going learn how to automatically run the Qunit tests using Grunt. Let's setup our project by adding the Qunit framework and tests file. Start with downloading the latest version from using bower.

.Installing qunit with bower
====
[source,bash]
----
bower install qunit
----
====
 

You need to get only two files - `qunit-*.js` and `qunit-*.css`.

.QUnit framework in our project 
image::images/ewdv_0702.png[]

.Our first QUnit test
====
[source,javascript]
----
include::include/ch7/first_qunit_test.js[]
----
====


You'll also need a test runner for the test setup. A test runner is an html file contains links to QUnit framework JavaScript file.

.A test runner
====
[source,html]
----
include::include/ch7/qunit-runner.htm[]
----
====

<1> In this section we continue working on the jQuery-based version of the Save The Child application. Hence our "production environment" depends on availability of jQuery, so we need to include jQuery in the test runner.

<2> Test files are included too.

<3> QUnit fills this block with results.

<4> Any HTML you want to be present in each test. It will be reset for each test.

To run all our tests we need to open the `qunit-runner.html` in browser.  (See <<FIG_8_06>>).

.Grunt config for qunit test runner
====
[source,javascript]
----
include::include/ch7/gruntfile_for_qunit.js[]
----
====

<1> Grunt loads task from local NPM repository. To install this tasks in +node_modules+ directory use command 'npm install grunt-contrib-qunit'.

.A test run results in browser
[[FIG_8_06]]
image::images/ewdv_0703.png[]

Now let's briefly review QUnit API components. You can find a typical QUnit script in <<QUNIT_TEST,the following listing>>.

[[QUNIT_TEST]]
.A sample QUnint test
====
[source,javascript]
----
include::include/ch7/sample_qunit_test.js[]
----
====

<1> A `module` function allows to combine related tests as group.

<2> Here we can run _Arrange_ phase. A `setup` function will be called before each test.

<3> A `teardown` function will be call after each test respectively. This is our _Reset_ phase.

<4> You need to place code of your test in corresponding `test` function.

<5> Typically, you need to use assertions to make sure the code being tested  gives expected results. The function http://api.qunitjs.com/ok/[`ok`] will examine the first argument to be `true`. 

<6> A pair of functions `equal` and `notEqual` will check for the equivalence of the first and second arguments, which could be expressions as well.

<7> A code of the test is wrapped in IIFE and passes `jQuery` object as `$` variable.

You can find more details about QUnit in http://api.qunitjs.com/[product  documentation] and http://qunitjs.com/cookbook/[QUnit Cookbook].         

==== Behavior-Driven Development With Jasmine

The idea behind _behavior-driven development_ (BDD) is to use the natural language constructs to describe what you think your code should be doing or more specifically, what your functions should be returning.

Similarly to unit tests, with BDD you write short specifications that test one feature at a time. Specifications should be sentences. For example, "Calculator adds two positive numbers". Such sentences will help you to easy identify the failed test by simply reading this sentence in the resulting report. 

Now we'll demonstrate this concept using Jasmine - the BDD frameworks for JavaScript. Jasmine provides a very nice way to group, execute, and report JavaScript unit tests. 

===== Setup Grunt with Jasmine

Now let's learn how to execute a Jasmine specification with Grunt. We will cover Jasmine basics in the next section, but for now think of Jasmine as a piece of code that should be executed by Grunt.

Let's start with downloading the latest version of Jasmine using bower. 

[source,bash]
----
bower install jasmine
----

Unzip `jasmine-standalone-2.0.0.zip` in _dist_ directory. Jasmine comes with an example spec (_spec_ folder) and an html test runner - SpecRunner.html. Let's open the file SpecRunner.html in browser <<FIG-2>>. 

.Running Jasmine Specs in a Browser
[[FIG-2]]
image::images/ewdv_0704.png["SpectRunner.html"]

The SpecRunner.html is structured similarly to QUnit html runner. You can run specifications by opening the runner file in browser.  

[source, html]
----------------------------------------------------------------------
include::include/ch7/SpecRunner.htm[]
----------------------------------------------------------------------

<1> Required Jasmine framework library.

<2> Initialize Jasmine and run all specifications when the page is loaded.

<3> Include the source files.

<4> Include the specification code. It's not required but files that contain specification code can have suffix `*Spec.js`.

Now let's update the Grunfile to run the same sample specifications with the PhantomJS headless browser. Copy the content of _src_ folder of your Jasmine distribution into the _app/js_ folder of our project, and then copy the content of the _spec_ folder into the _test/spec_ folder of your project. Also create a folder _test/lib/jasmine_ and copy the content of Jasmine distribution _lib_ folder there. <<FIG-3>>

.Jasmine Specifications in Our Project
[[FIG-3]]
image::images/ewdv_0705.png[]

Now you need to edit Gruntfile_jasmine.js to activate Jasmine support.

.Gruntfile_jasmine.js with Jasmine running support 
====
[source,javascript]
----------------------------------------------------------------------
include::include/ch7/gruntfile_for_jasmine.js[]
----------------------------------------------------------------------
====


<1> Configuring the Jasmine task.

<2> Specifying the location of the source files.

<3> Specifying the location of Jasmine specs.

<4> Specifying the location of Jasmine helpers, which will be covered later in this chapter.

<5> Grunt loads task from local NPM repository. To install this tasks in +node_modules+ directory use command 'npm install grunt-contrib-jasmine'.

To execute tests, run the command `grunt --gruntfile Gruntfile_jasmine.js jasmine`, and you should see something like this:

[source,bash]
----
Running "jasmine:src" (jasmine) task
Testing jasmine specs via phantom
.....
5 specs in 0.003s.
>> 0 failures

Done, without errors.
----

In this example, Grunt successfully executed the tests with <<WHAT_IS_PHANTOMJS,PhantomJS>> of all five specifications defined in PlayerSpec.js. 

[[WHAT_IS_CI]]
.What is Continuous Integration?
****
Continuous Integration (CI) is a software development practice where members of a team integrate their work frequently, which results in multiple integrations per day.

Introduced by Martin Fowler and Matthew Foemmel, the theory of continuous integration footnote:[http://martinfowler.com/articles/continuousIntegration.html[Continuous Integration by Martin Fowler]] recommends creating scripts and running automated builds (including tests) of your application at least once a day.
This allows you to identify issues in the code early.

Authors of this book successfully use an open source framework called http://jenkins-ci.org/[Jenkins] (there are other similar CI servers) for establishing continuous build process.

With Jenkins, you can have scripts that run either at a specified time interval or on each source code repository check-in of the new code. You may also force an additional build process whenever you like. The Grunt command line tool should be installed and be available on CI machine to allow the Jenkins server invoke Grunt scrips and publish test results.

We use it to ensure continuous builds of the internal and open source projects.

.Jenkins CI server running at savesichild.org website and used to build the sample applications for this book.
image::images/ewdv_0706.png[Jenkins]
****

In the next section you will learn how write your own specifications.

===== Jasmine Basics

After we've set up the tools for running tests, let's start developing tests and learn the Jasmine framework constructs. Every specification file has a set of _suites_ defined in the `describe` function. Suites help logically organize code of test specifications. 

.ExampleSpec.js
====
[source,javascript]
----
include::include/ch7/ExampleSpec.js[]
----
====


<1> The function `describe()` accepts two parameters - the name of the test suite, and the callback function. The function is a block of code that implements the suite. If for some reasons you would like to skip the suite from execution you can just use method `xdescribe()` and a whole suite will be excluded until you rename it back to `describe()`.

<2> The function `it()` also accepts similar parameters - the name of the test specification, and the function that implements this specification. Like in case with suites, Jasmine has a corresponding `xit` method to exclude specification from execution.

<3> Each suite can have any number of nested suites.

<4> Each suite can have any number of specifications.

<5> The code checks to see if `2+2` equals `4`. We used the function `toEqual()`, which is a _matcher_. Define expectations with the function `expect()`, which takes a value, called the actual. It's chained with a matcher function, which takes the expected value (in our case it's 4) and checks if it satisfies the criterion defined in the matcher. 

Various flavors of matchers are shipped with Jasmine framework, and we're going to review a couple the frequently used matchers functions.

* Equality
+

Function `toEqual()` checks if two things are equal.

* True or False?
+

Functions `toBeTruthy()` and `toBeFalsy()` checks if something is true or false respectively.

* Identity
+

Function `toBe()` checks if two things are _the same object_.

* Nullness
+

Function `toBeNull()` checks if something is `null`.

* Is Element Present
+

Function `toContain()` check if an actual value is an element of array.
+

[source, javascript]
----
expect(["James Bond", "Austin Powers", "Jack Reacher", "Duck"]).toContain("Duck");
----

* Negate Other Matchers
+

This function is used to reverse matchers to ensure that they aren't `true`. To do that, simply prefix things with `.not`:
+

[source, javascript]
----
expect(["James Bond", "Austin Powers", "Jack Reacher"]).not.toContain("Duck");
----

Above we've listed only some of existing matchers. You can find the complete documentation with code examples at http://jasmine.github.io/2.0/introduction.html#section-Matchers[official Jasmine website] and at https://github.com/pivotal/jasmine/wiki/Matchers[wiki].

TIP: There is a large set of jquery-specific matchers available https://github.com/velesin/jasmine-jquery

===== Specification Setup

Jasmine framework has an API to arrange your specification (based on <<AAAR>> concept). It includes two methods - `beforeEach()` and `afterEach()`, which allow you to execute some code before and after each spec respectively. It's very useful for instantiation of the shared objects or cleaning up after the tests complete.
If you need to fulfill your test with some common dependencies or setup the environment, just place code inside `beforeEach()` method. Such dependencies and environment are known as _fixture_.

****
*What is Fixture?*

Test fixture refers to the fixed state used as a baseline for running tests. The main purpose of a test fixture is to ensure that there is a well known and fixed environment in which tests are run so that results are repeatable. Sometimes a fixture also referred as _test context_.
****

.Specification setup with beforeEach
====
[source,javascript]
----
include::include/ch7/beforeEach.js[]
----
====


<1> This method will be called before each specification.

<2> In the `beforeEach()` method we create two input fields. These two inputs will be available in all specifications of this suite.

<3> You can place additional cleanup code inside `afterEach()` function.

<4> A `beforeEach()` function helps to implement _Don't Repeat Yourself_ principle in our tests. You don't need to create the dependency elements inside each specification manually.

<5> You can change defaults inside each specification without worrying about  affecting other specifications. Your test environment will be reset for each specification.


===== Custom Matchers

Jasmine framework is easily extensible, and it allows you to define your own matchers if for some reasons you're unable to find the appropriate matchers in the Jasmine distribution. In such cases you'd need to write a custom matcher. Let's write a matcher that check if a string contains name of the "secret agent" from the defined list of agents. 

.Custom `toBeSecretAgent` matcher
====
[source,javascript]
----
include::include/ch7/jasmine_custom_matcher.js[]
----
====


<1> We need to implement function `compare` that accepts two parameters from  `expect` call - actual and expected values.

<2> A function `compare` should return `result` object.

<3> This function checks if `agentsList` contains the actual value.

<4> A `pass` property of `result` object indicates success or failure of matcher execution;

<5> We can customize error message (a `message` property of `result` object) if the test fails.

The invocations of this helper can look like this:

[source,javascript]
----
it("part of super agents", function () {
    expect("James Bond").toBeSecretAgent();         // <1>
    expect("Jason Bourne").toBeSecretAgent();
    expect("Austin Powers").not.toBeSecretAgent();  // <2>
    expect("Austin Powers").toBeSecretAgent();     // <3>
});
----

<1> Calling the custom matcher.

<2> Custom matchers could be used together with the `.not` modifier.

<3> This expectation will fail because 'Austin Powers' is not in the list of secret agents.

The following custom failure message will be displayed on the console.
    
[source,bash]
----
grunt --gruntfile Gruntfile_jasmine.js test
Running "jasmine:src" (jasmine) task
Testing jasmine specs via PhantomJS
 My function under test should
   ✓ return on
   another suite
     ✓ spec1
   ✓ my another spec
   ✓ 2+2 = 4
   X part of super agents
     Austin Powers is not a secret agent (1)

5 specs in 0.01s.
>> 1 failures
Warning: Task "jasmine:src" failed. Use --force to continue.

Aborted due to warnings.
----

"Austin Powers is not a secret agent (1)" is a custom failure message.

===== Spies

Test spies are objects that replace the actual functions with the code to record information about the function's usage through the systems being tested. Spies are useful when determining a function's success is not easily accomplished by inspecting its return value or changes to the state of objects with witch it interacts. 

Consider the following example of login functionality. A `showAuthorizedSection()` function will be invoked within `login` function after the user entered the correct user name and password. We need to test that the invocation of the `showAuthorizedSection()` is happening in this sequence.

[[LOGIN_SNIP]]
.Production code of `login` function.
====
[source,javascript]
----
var ssc = {};
(function() {
    'use strict';
    ssc.showAuthorizedSection = function() {
        console.log("showAuthorizedSection");
    };
    ssc.login = function(usernameInput, passwordInput) {
        // username and password check logic is omitted
        this.showAuthorizedSection();
    };
})();
----
====


And here is how we can test it using Jasmine's spies.

[source,javascript]
----
describe("login module", function() {
    it("showAuthorizedSection has been called", function() {
        spyOn(ssc, "showAuthorizedSection"); // <1>
        ssc.login("admin", "1234"); // <2>
        expect(ssc.showAuthorizedSection).toHaveBeenCalled(); // <3>
    });
});
----
<1> The `spyOn` function will replace `showAuthorizedSection()` function with corresponded spy.

<2> The `showAuthorizedSection()` function will be invoked within `login()` function in case of successful login.

<3> Assertion `toHaveBeenCalled()` would be not possible without spy.


// ===== BDD Best Practices
// TBD
// Organize Tests As If Reading a Story
// Test method names should be sentences
// Developers discovered it could do at least some of their documentation for them, so they started to write test methods that were real sentences. What's more, they found that when they wrote the method name in the language of the business domain, the generated documents made sense to business users, analysts, and testers.
// A simple sentence template keeps test methods focused 
// An expressive test name is helpful when a test fails 

==== Multi-Browser Testing

The previous section was about executing your test and specification in a headless mode using Grunt and PhantomJS, which is very useful for running tests in the CI environments. While PhantomJS uses WebKit rendering engine, there are browsers that don't use WebKit. It's obvious that running tests manually in each browser is tedious and not productive.
To automate testing in all Web browsers, you can use Testem Runner. Testem executes your tests, analyzes its output and prints result on the console. In this section you'll learn how to install and configure `Testem` to run Jasmine tests.

===== Installation

Testem uses Node.js APIs and can be installed with NPM:

[source,bash]
----
npm install testem -g
----

===== Testem Configuration file

Testem runner will just pick any of JavaScript in your project directory. If testem can identify any tests among that _.js_ files it will run it.
But Testem tasks can be customized using a configuration file. 

You can configure Testem to specify which files should be included in testing. Testem starts with trying to find the configuration file _testem.json_ in the project directory. A sample `testem.json` is shown in <<TESTEM_JSON,following listing>>. 

[[TESTEM_JSON]]
.A Testem Configuration File
====
[source,javascript]
----
{
    "framework": "jasmine2",         // <1>
    "src_files": [                 // <2> 
        "ext/ext-all.js",
        "test.js"
    ]
}
----
====

<1> The `framework` directive is used to specify the test framework. Testem supports QUnit, Jasmine and many more frameworks. You can find full list of supported frameworks on testem https://github.com/airportyh/testem#features[github page].

<2>  The list of test and production code source files. 

===== Running Tests 

Testem supports two running modes: test-driven development mode (_tdd-mode_)  and continuous integration (_ci-mode_). (For more about continuous integration, see the note on <<WHAT_IS_CI,CI>>. In tdd-mode, testem starts the development server.

.Testem tdd-mode.
image::images/ewdv_0707.png[]

In tdd-mode, Testem doesn't spawn any browser automatically. On the contrary, you'd need to open this url in the browser you want run test against to connect it to Testem server. From this point on, Testem will execute tests in all connected browsers.  On the next <<TESTEM_MULTI,screeshot>> you can see we added different browsers including mobile version of Safari (running on iOS simulator).

[[TESTEM_MULTI]]
.Testem is running the tests on the multiple browsers.
image::images/ewdv_0708.png[]

Because the Testem server itself is an HTTP server, you can connect remote browsers to it as well. For example, the <<TESTEM_IE, following screenshot>> shows Internet Explorer 10 running under Windows 7 virtual machine connected to the testem server.

[[TESTEM_IE]]
.Using testem to test code on remote IE 10
image::images/ewdv_0709.png[]

Running the tests with testem runner can be combined with previously introduced Grunt tool. The <<TESTEM_AND_GRUNT,next screenshot>> shows two tests in parallel: testem runs tests on the real browsers and and grunt runs tests on the headless PhantomJS.

[[TESTEM_AND_GRUNT]]
.Using testem and `grunt watch` side-by-side 
image::images/ewdv_0710.png[]

Testem supports live reloading mode. This means that Testem will watch file system for changes and will execute tests in all connected browsers automatically. You can force to test run by switching to console and hitting the _Enter_ key.

In CI mode testem will examine system for all of the available browsers in your system and will execute tests on it. You can get list of the browsers that testem can use to run tests with the `testem launchers` command. <<TESTEM_L,Here is sample output>> after running this command. 

[[TESTEM_L]]
----
# testem launchers
Have 5 launchers available; auto-launch info displayed on the right.

Launcher      Type          CI  Dev
------------  ------------  --  ---
Chrome        browser       ✔
Firefox       browser       ✔
Safari        browser       ✔
Opera         browser       ✔
PhantomJS     browser       ✔
----

Now you can run our test simultaneously in all browsers installed in  your computer - Google Chrome, Safari, Firefox, Opera and PhantomJS - with one command:

`testem ci`

.An output of `testem ci` command
====
[source,bash]
----
# Launching Chrome      # <1>
# 

# Launching Firefox     # <2>
# ....
TAP version 13
ok 1 - Firefox Basic Assumptions:  Ext namespace should be available loaded.
ok 2 - Firefox Basic Assumptions:  ExtJS 4.2 should be loaded.
ok 3 - Firefox Basic Assumptions:  SSC code should be loaded.
ok 4 - Firefox Basic Assumptions:  something truthy.

# Launching Safari      # <3>
# 

# Launching Opera       # <4>
# ....
ok 5 - Opera Basic Assumptions:  Ext namespace should be available loaded.
ok 6 - Opera Basic Assumptions:  ExtJS 4.2 should be loaded.
ok 7 - Opera Basic Assumptions:  SSC code should be loaded.
ok 8 - Opera Basic Assumptions:  something truthy.

# Launching PhantomJS       # <5>
# 

1..8
# tests 8
# pass  8

# ok
....
----
====

<1> The tests are run on Chrome...
<2> ... Firefox
<3> ... Safari 
<4> ... Opera
<5> ... and on headless WebKit - PhantomJS

Testem uses http://en.wikipedia.org/wiki/Test_Anything_Protocol[TAP] format to report test results. 

==== Testing DOM

As we discussed in Chapter 1, Document Object Model is a standard browser API that allows a developer to access and manipulate page elements. Pretty often your JavaScript code needs to access and manipulate the HTML page elements in some way. Testing DOM is the crucial part of testing your client side JavaScript. 
By design, the DOM standard defines a browser-agnostic API. But in the real world, if you want to make sure that your code works in the particular browser you need to run the test inside this browser.

Earlier in this chapter we've introduced the Jasmine method `beforeEach()`, which is the right place for setting all required DOM elements and making them available in the specifications.

[[FIX-1]]
.Using jQuery APIs to create the required DOM elements before run the spec
====
[source,javascript]
----
describe("spec", function() {
    var usernameInput;
    beforeEach(function() { // <1>
        usernameInput = $(document.createElement("input")).attr({ // <2>
            type: 'text',
            id: 'username',
            name: 'username'
        })[0];
    });
});
----
====

<1> Inside the `beforeEach()` method we're using the API to manipulate the DOM programatically. Also, if you're using an HTML test runner you can add the fixture using HTML tags. But we don't recommend this approach because pretty soon you will find that the test runner will become unmaintainable and clogged with tons of fixture HTML code.

<2> Create an `<input>` element using jQuery APIs, which will turn into the following HTML:   

[source,html]
----
<input type=​"text" id=​"password" name=​"password" placeholder=​"password" autocomplete=​"off">​
----

The jQuery selectors API is more convenient for working with DOM than a standard JavaScript DOM API. But in the future examples we will use the https://github.com/searls/jasmine-fixture[jasmine-fixture] library for easier setup of the DOM fixture. Jasmine-fixture uses similar to jQuery selectors syntax for injecting HTML fixtures. With this library you will significantly decrease the amount of repetitive code while creating the fixtures.

// TODO: add jasmine-fixture setup, how to download and add to jasmine Grunts's task

Let's see how the example from <<FIX-1,previous code snippet>> looks like with the jasmine-fixture library.

.Using jasmine-fixture to setup the DOM before spec run
====
[source,javascript]
----
describe("spec", function() {
    var usernameInput;
    beforeEach(function() {
        usernameInput = affix('input[id="username"][type="text"][name="username]')[0]; // <1>
    });

    it("should not allow login with empty username and password and return code equals 0", function() {
        var result = ssc.login(usernameInput, passwordInput); // <2>
        expect(result).toBe(0);
    });
});
----
====

<1> Using the `affix()` function provided by the _jasmine-fixture_ library and expressiveness of CSS selectors we can easily setup required DOM elements. More examples of possible selectors could be found at the https://github.com/searls/jasmine-fixture#more-examples[documentation page] of jasmine-fixture.

<2> Now when all requirements for our production code (`login()` function) are satisfied we can run it in the context of a test and assert the results.

As you can see, testing of the DOM manipulation code is much like any other type of unit testing. You need to prepare a fixture (a.k.a. the testing context), run the production code and assert the results.

// TBD Ext JS create fixture div tag, render extjs component on it

// TBD faking events and user interaction
// TBD using spies to mock code related to interaction with server

=== Save The Child With TDD

==== The Test-Driven ExtJS version of _SaveSickChild.org_

We assume that you've read the materials from Chapter 6, and in this section you'll apply your newly acquired Ext JS skills. As a reminder, Ext JS framework encourages using MVC architecture. The separation of responsibilities between Views, Models and Controllers makes an ExtJS application a perfect candidate for unit testing. In this section you'll learn how to test the Ext JS version of the Save The Child application from Chapter 6.

===== Harnessing ExtJS application

Let's create a skeleton application that can provide for our classes under the test familiar environment. 

.A HTML-runner for Jasmine and ExtJS application
====
[source,html]
----
include::include/ch7/jasmine_runner_for_extjs.htm[]
----
====

<1> Adding ExtJS framework dependencies.

<2> Adding the Jasmine framework dependencies.

<3> This is our skeleton ExtJS application that will setup "friendly" environment for components under the test. You can see content of test.js in the <<SKELETON_EXTJS_APP,following listing>>.

[[SKELETON_EXTJS_APP]]
.A ExtJS testing endpoint
====
[source,javascript]
----
include::include/ch7/jasmine_extjs_runner.js[]
----
====

<1> Ext JS loader needs to know the location of the testing classes...

<2> ... and about location of production code.

<3> Create a skeleton application in the namespace of the production code to provide the execution environment.

<4> The `AllSpec` class will be requesting loading of the rest of the specs. We will show code of `AllSpec` class in <<ALL_SPEC,next listing>>

<5> The skeleton application will test the controllers from the production application code

[[ALL_SPEC]]
.AllSpec class
[source,javascript]
----
Ext.define('Test.spec.AllSpecs', {
    requires: [         // <1>
        'Test.spec.BasicAssumptions'
    ]
});
----
<1> The `requires` property includes an array of Jasmine suites. All further tests will be added to this array. Ext JS framework will be responsible for loading and instantiation all test classes.

Here is how our typical test suite will be look like.

.A BasicAssumptions class
====
[source,javascript]
----
Ext.define('Test.spec.BasicAssumptions', {}, function() {   // <1>
    describe("Basic Assumptions: ", function() {            // <2>
        it("Ext namespace should be available loaded", function() {
            expect(Ext).toBeDefined();
        });
        it("SSC code should be loaded", function() {
            expect(SSC).toBeDefined();
        });
    });
});
----
====

<1> Wrap the Jasmine suite into an Ext JS class.

<2> The rest of the code is very similar to the Jasmine code sample shown earlier in this chapter.

After setting up the testing harness for Save The Child application we will suggest testing strategy for ExtJS applications. Let's start with testing the models and controllers followed by testing the views.

[[testing_the_models]]
===== Testing the Models

_SaveSickChild.org_ home page displays the information about fund raising campaigns using chart and table views backed by collection of `Campaign` models. A `Campaign` model should have three properties: `title`, `description`, and `location`. The `title` property of the model should have a default value - `Default Campaign Title`. The `location` property of the model is required field.

In a spirit of TDD, let's write the <<CAMPMOD_SPEC,specification>> what will meet the requirements described above.

[[CAMPMOD_SPEC]]
.CampaignModelAssumptions specification
====
[source,javascript]
----
include::include/CampaignModelAssumptions.js[]
----
====

<1> By default, `Ext.data.Model` caches every model created by the application in a global in-memory array. We need to clean up the ExtJS model cache after each test run.

<2> Instantiate the `Campaign` model class to check that it exists.

<3> Next we need to check if model has all required properties. 

<4> The property `title` has a default value.

<5> Validation will fail on the empty `location` property.

[source,javascript]
----
include::include/Campaign.js[]
----


[[CONTROLLER_TEST]]
===== Testing The Controllers

Controllers in ExtJS are classes like any others and should be tested the same way. In the next example, let's test _Donate Now_ functionality. When the user clicks _Donate Now_ button of the `Donate` panel controller's code should validate the user input and submit the data to the server. Since we are just  testing controller's behavior, we're not going to submit the actual data. We'll use Jasmine spies instead. 

.Donate controller specification
====
[source,javascript]
----
Ext.define("Test.spec.DonateControllerSpec", {}, function () {
    describe("Donate controller", function () {
        beforeEach(function () {
            // controller's setup code is omitted
        });
        it('should exists', function () {       // <1>
            var controller = Ext.create('SSC.controller.Donate');
            expect(controller.$className).toEqual('SSC.controller.Donate');
        });
        describe('donateNow button', function () {
            it('calls donate on DonorInfo if form is valid', function () {
                var donorInfo = Ext.create('SSC.model.DonorInfo', {});
                var donateForm = Ext.create('SSC.view.DonateForm', {});
                var controller = Ext.create('SSC.controller.Donate');
                spyOn(donorInfo, 'donate');     // <2>
                spyOn(controller, 'getDonatePanel').and.callFake(function () {   // <3>
                    donateForm.down = function () {
                        return {
                            isValid: function () {
                                return true;
                            },
                            getValues: function () {
                                return {};
                            }
                        };
                    };
                    return donateForm;
                });
                spyOn(controller, 'newDonorInfo').and.callFake(function () { //<4>
                    return donorInfo;
                });
                controller.submitDonateForm();  
                expect(donorInfo.donate).toHaveBeenCalled();    // <5>
            });
        });
    });
});
----
====

<1> First, you need to test if controller's class is available and can be instantiated.

<2> With the help of Jasmine's `spyOn()` function substitute the `DonorInfo` model's `donate()` function. 

<3> We're not interested in the view's interaction -  only the contract should be tested. At this point, some methods can be substituted with the fake implementation to let the test pass. In this case, the specification tests the situation when form's valid.

<4> Next you need to inject emulated controller dependencies. The function `donate()` was replaced by the spy.

<5> Finally, you can assert if the function was called by the controller.

The function under the test looks as follows:

[source,javascript]
----
Ext.define('SSC.controller.Donate', {
    extend: 'Ext.app.Controller',
    refs: [{
            ref: 'donatePanel',
            selector: '[cls=donate-panel]'
        }
    ],
    init: function() {
        'use strict';
        this.control({
            'button[action=donate]': {
                click: this.submitDonateForm
            }
        });
    },
    newDonorInfo: function() {          // <1>
        return Ext.create('SSC.model.DonorInfo', {});
    },
    submitDonateForm: function() {
        var form = this.getDonatePanel().down('form');      
        if (form.isValid()) {       //<2>
            var donorInfo = this.newDonorInfo();
            Ext.iterate(form.getValues(), function(key, value) {  //<3>
                donorInfo.set(key, value);
            }, this);
            donorInfo.donate();   // <4>
        }
    }
});
----
<1> The factory method for creating a new instance of the `SSC.model.DonorInfo` class.

<2> If the form is valid, read data from the form fields...

<3> ... and populate properties of corresponding object.

<4> `DonorInfo` can be submitted by calling the `donate()` method.

===== Testing The Views

UI Tests can be divided into two constituent parts: interaction tests and component tests.Interactions tests simulate real-world scenarios of application usage as if a user is using the application. It's better to delegate the interaction tests to the functional testing tools like Selenium or CasperJS.

****
There is another UI testing tool worth to mention especially, in the context of testing ExtJS applications - http://bryntum.com/products/siesta/[Siesta]. Siesta allows to perform testing of the DOM and simulate user interactions. Siesta written in JavaScript and uses Siesta for unit and UI testing. There are two editions of Siesta - lite and professional. 
****

Component Tests isolate independent and reusable pieces of your application to verify their display, behavior and contract with other components (see <<CONTROLLER_TEST,testing of the controllers>>). Let's see how we can do that. Consider <<EXT_VIEW_TEST,following example>>

[[EXT_VIEW_TEST]]
.Example Title Needed Here
====
[source,javascript]
----
Ext.define('Test.spec.ViewsAssumptions', {}, function () {
    function prepareDOM(obj) {      //<1>
        Ext.DomHelper.append(Ext.getBody(), obj);
    }
    describe('DonateForm ', function () {
        var donateForm = null;  //<2>
        beforeEach(function () {
            prepareDOM({tag: 'div', id: 'test-donate'}); // <3>
            donateForm = Ext.create('SSC.view.DonateForm', {    //<4>
                renderTo: 'test-donate'
            });
        });
        afterEach(function () {
            donateForm.destroy();   // <5>
            donateForm = null;
        });
        it('should have donateform xtype', function () {
            expect(donateForm.isXType('donateform')).toEqual(true); // <6>
        });
    });
});
----
====
<1> A helper function for fixture DOM elements creation.

<2> A reusable scoped variable.

<3> Create fixture `test` div.

<4> Create a fresh form for every test to avoid test pollution.

<5> Destroy the form after every test so we don't pollute the environment.

<6> In this test, you need to make sure that the `DonateForm` component has `donateform` xtype.


==== Setting up the IDE for TDD

In this section we will setup WebStorm to use the described above tools inside this IDE. We will show how to integrate Grunt tool with WebStrom to run grunt tasks from there. 

===== Integrate the Grunt Tool with WebStorm

Let's start with the Grunt setup. Currently, there is no native support of the Grunt tool in WebStorm IDE. Since Grunt is a command line tool, you can use a general launching feature of the WebStorm IDE and configure it as an _External Tool_. Open the WebStorm preferences and navigate to _External Tools_ section to get access to the external tools configuration as in <<FIG_08_08>>.

.External Tools configuration window in WebStorm
[[FIG_08_08]]
image::images/ewdv_0711.png[]

Click the `+` button to create new External Tool configuration.

.External Tool configuration
image::images/ewdv_0712.png[]

1. You need to specify the full path to the application executable.

2. Some tools require command line parameters. In this example, we explicitly specify the task runner configuration file (with the `--gruntifle` command line option) and the task to be executed.

3. Also you need to specify the _Working Directory_ to run the Grunt tool. In our case, grunt configuration file is located in the root of our project. WebStorm allows to use macros to avoid hard-coded paths. Most likely, you don't want to setup external tools for each new project, just create a universal setup. In our example we use the `$ProjectFileDir$` macros will be resolved as current WebStorm project folder root.

4. WebStorm allows you to organize related tasks into logical groups.

5. You can configure how to access the external tool launcher.

When all of the above steps are complete you can find the launcher under _Tools_ menu as well under Main menu Editor menu, Project views and etc.  

.Grunt launcher available under _Tools/grunt_ menu
image::images/ewdv_0713.png[]

Unit tests are really important as a mean to get a quick feedback from your code. You can work more efficient if you manage to minimize context switching during your coding flow. Also, you don't want to waste time digging through the menu items of your IDE, so assigning a keyboard shortcut for launching external tool is a good idea.

Let's assign a keyboard shortcut for our newly configured external tool launcher. Go to the _Keymap_ section in WebStorm Preferences. Use the filter to find our created launcher `jasmine: grunt test`. Specify either the Keyboard of the Mouse shortcut by double clicking on the appropriate list item.

.Setup keyboard shortcut for grunt launcher
image::images/ewdv_0714.png[]

By pressing a combination of keys specified in the previous screen, you will be able to launch the grunt with Jasmine tests with one click of a button(s). WebStorm will redirect all the output from the grunt tool into its  Run window.

.Grunt output in WebStorm
image::images/ewdv_0715.png[]

=== Summary

Testing is one of the most important processes of software development. Well organized testing helps keeping the code in a good and working state. It's especially important in interpreted languages like JavaSctipt where there is no compiler to provide a helping hand to find lots of errors on very early stages. 

In this situation, static code analysis tools, like JSHint (discussed in «Selected Productivity Tools for Enterprise Developers» chapter), could become very handy in helping with identifying typos and enforcing best practices accepted by the JavaScript community.

In enterprise projects developed with compiled languages people often debate if test-driven development is really beneficial. With JavaScript it's non-debatable unless you have unlimited time and budget and are ready to live with unmaintainable JavaScript. 

The enterprises that have adopted test-driven development (as well as behavior-driven development) routines make the application development process safer by including test scripts in the continuous integration build process. 

Automating unit tests reduces the number of bugs and decreases the amount of time developers need to spend manually testing their code. If automatically launched test scripts (unit, integration, functional, and load testing) don't reveal any issues, you can rest assured that the latest code changes did not break the application logic, and that the application performs according to SLA. 

[sect2]
=== References

[[[fowler]]] Martin Fowler. 'Refactoring. Improving the Design of Existing Code' 2002 p.212